{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "096bf144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a504ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_strided_conv(layer):\n",
    "    # Detect a Conv1d with stride > 1\n",
    "    ks = getattr(layer, \"kernel_size\", None)\n",
    "    st = getattr(layer, \"stride\", None)\n",
    "    if ks and st:\n",
    "        # kernel_size/stride may be tuples\n",
    "        return any(s > 1 for s in (st if isinstance(st, (list,tuple)) else (st,)))\n",
    "    return False\n",
    "\n",
    "def collapse_summary_layers(layers):\n",
    "    groups = []\n",
    "    enc_id = 0\n",
    "    dec_id = 0\n",
    "    pending = None\n",
    "    last_was_convT = False\n",
    "\n",
    "    for L in layers:\n",
    "        cls = L.layer_class.__name__ if hasattr(L, \"layer_class\") else type(L).__name__\n",
    "\n",
    "        # 1) Encoder block start\n",
    "        if is_strided_conv(L):\n",
    "            # finish previous if any\n",
    "            if pending:\n",
    "                groups.append(pending)\n",
    "            enc_id += 1\n",
    "            pending = {\n",
    "                \"Stage\": f\"Encoder Block {enc_id}\",\n",
    "                \"layers\": [],\n",
    "                \"input_size\": L.input_size,\n",
    "                \"params\": 0\n",
    "            }\n",
    "            # fall through to append this conv\n",
    "\n",
    "        # 2) Bottleneck LSTM (once)\n",
    "        elif \"LSTM\" in cls and pending is None and enc_id and dec_id==0:\n",
    "            # finish any pending (shouldn't be)\n",
    "            if pending:\n",
    "                groups.append(pending)\n",
    "            # create LSTM group\n",
    "            groups.append({\n",
    "                \"Stage\": \"Bottleneck LSTM\",\n",
    "                \"layers\": [\"LSTM\"],\n",
    "                \"input_size\": L.input_size,\n",
    "                \"output_size\": L.output_size,\n",
    "                \"params\": L.num_params\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # 3) Decoder block start\n",
    "        elif cls == \"ConvTranspose1d\":\n",
    "            # finish previous if any\n",
    "            if pending:\n",
    "                groups.append(pending)\n",
    "            dec_id += 1\n",
    "            pending = {\n",
    "                \"Stage\": f\"Decoder Block {dec_id}\",\n",
    "                \"layers\": [],\n",
    "                \"input_size\": L.input_size,\n",
    "                \"params\": 0\n",
    "            }\n",
    "            last_was_convT = True\n",
    "            # fall through to append\n",
    "\n",
    "        # 4) within a block: nothing special\n",
    "        if pending:\n",
    "            # add this layer\n",
    "            pending[\"params\"] += getattr(L, \"num_params\", 0)\n",
    "\n",
    "            # build a short descriptor\n",
    "            if cls == \"Conv1d\":\n",
    "                k = L.kernel_size[-1] if hasattr(L, \"kernel_size\") else \"?\"\n",
    "                s = L.stride[-1]    if hasattr(L, \"stride\")    else \"\"\n",
    "                pending[\"layers\"].append(f\"Conv1d(k={k},s={s})\")\n",
    "            elif cls == \"ConvTranspose1d\":\n",
    "                k = L.kernel_size[-1] if hasattr(L, \"kernel_size\") else \"?\"\n",
    "                s = L.stride[-1]    if hasattr(L, \"stride\")    else \"\"\n",
    "                pending[\"layers\"].append(f\"ConvT1d(k={k},s={s})\")\n",
    "            elif \"ReLU\" in cls:\n",
    "                pending[\"layers\"].append(\"ReLU\")\n",
    "                if last_was_convT:\n",
    "                    # treat this ReLU as end of decoder block\n",
    "                    pending[\"output_size\"] = L.output_size\n",
    "                    groups.append(pending)\n",
    "                    pending = None\n",
    "                    last_was_convT = False\n",
    "            elif \"GLU\" in cls:\n",
    "                pending[\"layers\"].append(\"GLU\")\n",
    "                # end of encoder block\n",
    "                pending[\"output_size\"] = L.output_size\n",
    "                groups.append(pending)\n",
    "                pending = None\n",
    "\n",
    "    # if something left hanging, append it\n",
    "    if pending:\n",
    "        pending[\"output_size\"] = pending.get(\"output_size\", pending[\"input_size\"])\n",
    "        groups.append(pending)\n",
    "\n",
    "    # build DataFrame\n",
    "    records = []\n",
    "    for g in groups:\n",
    "        records.append({\n",
    "            \"Stage\":       g[\"Stage\"],\n",
    "            \"Layers\":      \" → \".join(g[\"layers\"]),\n",
    "            \"Input Shape\": g[\"input_size\"],\n",
    "            \"Output Shape\":g[\"output_size\"],\n",
    "            \"Params\":      g[\"params\"],\n",
    "        })\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5376da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_def import CausalDemucsSplit\n",
    "\n",
    "def main():\n",
    "    # 1) Instantiate and summarize\n",
    "    model = CausalDemucsSplit().eval()\n",
    "    summ = summary(\n",
    "        model,\n",
    "        input_size=(1, 1, 32085),\n",
    "        verbose=0,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "        depth=None\n",
    "    )\n",
    "\n",
    "    # 2) Grab the summary list\n",
    "    layers = getattr(summ, \"summary_list\", None) or getattr(summ, \"_summary_list\", [])\n",
    "\n",
    "    # 3) Define the order in which blocks appear, with a prefix to match against each layer's name\n",
    "    block_prefixes = [\n",
    "        (\"Input Upsample\", \"sinc_interpolation\"),          # your upsampling conv\n",
    "        (\"Encoder Block 1\", \"encoder.layers.0\"),\n",
    "        (\"Encoder Block 2\", \"encoder.layers.1\"),\n",
    "        (\"Encoder Block 3\", \"encoder.layers.2\"),\n",
    "        (\"Encoder Block 4\", \"encoder.layers.3\"),\n",
    "        (\"Encoder Block 5\", \"encoder.layers.4\"),\n",
    "        (\"Bottleneck LSTM\",      \"lstm\"),\n",
    "        (\"Decoder Block 5\", \"decoder.layers.4\"),\n",
    "        (\"Decoder Block 4\", \"decoder.layers.3\"),\n",
    "        (\"Decoder Block 3\", \"decoder.layers.2\"),\n",
    "        (\"Decoder Block 2\", \"decoder.layers.1\"),\n",
    "        (\"Decoder Block 1\", \"decoder.layers.0\"),\n",
    "        (\"Output Conv\", \"output_conv\")                    # your final deconv or conv\n",
    "    ]\n",
    "\n",
    "    # 4) Iterate and group\n",
    "    grouped = []\n",
    "    current = None\n",
    "    for layer in layers:\n",
    "        name = layer.name  # e.g. \"encoder.layers.0.conv1\"\n",
    "        # see if this layer starts a new block\n",
    "        for label, prefix in block_prefixes:\n",
    "            if name.startswith(prefix):\n",
    "                # start a new group\n",
    "                if current:\n",
    "                    grouped.append(current)\n",
    "                current = {\n",
    "                    \"Block\": label,\n",
    "                    \"layers\": [],\n",
    "                    \"Params\": 0\n",
    "                }\n",
    "                break\n",
    "        if not current:\n",
    "            # skip any stray layers before the first block\n",
    "            continue\n",
    "        # add this layer into the current block\n",
    "        current[\"layers\"].append(layer)\n",
    "        current[\"Params\"] += layer.num_params\n",
    "\n",
    "    # add the final block\n",
    "    if current:\n",
    "        grouped.append(current)\n",
    "\n",
    "    # 5) Build table rows\n",
    "    rows = []\n",
    "    for g in grouped:\n",
    "        first, last = g[\"layers\"][0], g[\"layers\"][-1]\n",
    "\n",
    "        # build a short operation summary\n",
    "        ops = []\n",
    "        for l in g[\"layers\"]:\n",
    "            lt, k, s = get_layer_type_and_params(l)\n",
    "            # only show params if it's a conv or deconv or GLU\n",
    "            if lt in (\"Conv1d\", \"ConvTranspose1d\"):\n",
    "                ops.append(f\"{lt}(k={k},s={s})\")\n",
    "            elif lt in (\"GLU\", \"ReLU\", \"LSTM\"):\n",
    "                ops.append(lt)\n",
    "            else:\n",
    "                # skip BatchNorm, Dropout, etc. or include if you like\n",
    "                ops.append(lt)\n",
    "        op_str = \"→\".join(ops)\n",
    "\n",
    "        rows.append({\n",
    "            \"Block\":         g[\"Block\"],\n",
    "            \"Operation\":     op_str,\n",
    "            \"Output Shape\":  last.output_size,\n",
    "            \"Params\":        g[\"Params\"]\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # 6a) Print Markdown table\n",
    "    print(df.to_markdown(index=False))\n",
    "\n",
    "    # 6b) Save CSV if you want\n",
    "    df.to_csv(\"model_summary_grouped.csv\", index=False)\n",
    "    print(\"\\nSaved → model_summary_grouped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da429c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c79e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "denoising",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
